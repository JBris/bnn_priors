{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numbers import Number\n",
    "import torch\n",
    "from torch.distributions import constraints, Gamma\n",
    "from torch.distributions.distribution import Distribution\n",
    "from torch.distributions.utils import broadcast_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Laplace(Distribution):\n",
    "    r\"\"\"\n",
    "    Creates a Laplace distribution parameterized by :attr:`loc` and :attr:`scale`.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "        >>> m.sample()  # Laplace distributed with loc=0, scale=1\n",
    "        tensor([ 0.1046])\n",
    "\n",
    "    Args:\n",
    "        loc (float or Tensor): mean of the distribution\n",
    "        scale (float or Tensor): scale of the distribution\n",
    "    \"\"\"\n",
    "    arg_constraints = {'loc': constraints.real, 'scale': constraints.positive}\n",
    "    support = constraints.real\n",
    "    has_rsample = True\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.loc\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        return 2 * self.scale.pow(2)\n",
    "\n",
    "    @property\n",
    "    def stddev(self):\n",
    "        return (2 ** 0.5) * self.scale\n",
    "\n",
    "    def __init__(self, loc, scale, validate_args=None):\n",
    "        self.loc, self.scale = broadcast_all(loc, scale)\n",
    "        if isinstance(loc, Number) and isinstance(scale, Number):\n",
    "            batch_shape = torch.Size()\n",
    "        else:\n",
    "            batch_shape = self.loc.size()\n",
    "        super(Laplace, self).__init__(batch_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(Laplace, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        new.loc = self.loc.expand(batch_shape)\n",
    "        new.scale = self.scale.expand(batch_shape)\n",
    "        super(Laplace, new).__init__(batch_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        finfo = torch.finfo(self.loc.dtype)\n",
    "        if torch._C._get_tracing_state():\n",
    "            # [JIT WORKAROUND] lack of support for .uniform_()\n",
    "            u = torch.rand(shape, dtype=self.loc.dtype, device=self.loc.device) * 2 - 1\n",
    "            return self.loc - self.scale * u.sign() * torch.log1p(-u.abs().clamp(min=finfo.tiny))\n",
    "        u = self.loc.new(shape).uniform_(finfo.eps - 1, 1)\n",
    "        # TODO: If we ever implement tensor.nextafter, below is what we want ideally.\n",
    "        # u = self.loc.new(shape).uniform_(self.loc.nextafter(-.5, 0), .5)\n",
    "        return self.loc - self.scale * u.sign() * torch.log1p(-u.abs())\n",
    "\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        return -torch.log(2 * self.scale) - torch.abs(value - self.loc) / self.scale\n",
    "\n",
    "\n",
    "    def cdf(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        return 0.5 - 0.5 * (value - self.loc).sign() * torch.expm1(-(value - self.loc).abs() / self.scale)\n",
    "\n",
    "\n",
    "    def icdf(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        term = value - 0.5\n",
    "        return self.loc - self.scale * (term).sign() * torch.log1p(-2 * term.abs())\n",
    "\n",
    "\n",
    "    def entropy(self):\n",
    "        return 1 + torch.log(2 * self.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace = Laplace(0.,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0931)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplace.log_prob(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedNormal(Distribution):\n",
    "    r\"\"\"\n",
    "    Creates a Generalized Normal distribution parameterized by :attr:`loc`, :attr:`scale`, and :attr:`beta`.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> m = GeneralizedNormal(torch.tensor([0.0]), torch.tensor([1.0]), torch.tensor(0.5))\n",
    "        >>> m.sample()  # GeneralizedNormal distributed with loc=0, scale=1, beta=0.5\n",
    "        tensor([ 0.1337])\n",
    "\n",
    "    Args:\n",
    "        loc (float or Tensor): mean of the distribution\n",
    "        scale (float or Tensor): scale of the distribution\n",
    "        beta (float or Tensor): shape parameter of the distribution\n",
    "    \"\"\"\n",
    "    arg_constraints = {'loc': constraints.real, 'scale': constraints.positive, 'beta': constraints.positive}\n",
    "    support = constraints.real\n",
    "    has_rsample = True\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.loc\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        return (self.scale.pow(2) * torch.lgamma(3/self.beta).exp()) / torch.lgamma(1/self.beta).exp()\n",
    "\n",
    "    @property\n",
    "    def stddev(self):\n",
    "        return self.variance()**0.5\n",
    "\n",
    "    def __init__(self, loc, scale, beta, validate_args=None):\n",
    "        self.loc, self.scale = broadcast_all(loc, scale)\n",
    "        self.beta = beta\n",
    "        gamma_dist = Gamma(concentration=(1/self.beta), rate=(self.scale**self.beta)) # is this rate right?\n",
    "        self.gamma_cdf = gamma_dist.cdf\n",
    "        self.gamma_icdf = gamma_dist.icdf\n",
    "        if isinstance(loc, Number) and isinstance(scale, Number):\n",
    "            batch_shape = torch.Size()\n",
    "        else:\n",
    "            batch_shape = self.loc.size()\n",
    "        super(GeneralizedNormal, self).__init__(batch_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(GeneralizedNormal, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        new.loc = self.loc.expand(batch_shape)\n",
    "        new.scale = self.scale.expand(batch_shape)\n",
    "        super(GeneralizedNormal, new).__init__(batch_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):  # TODO\n",
    "        # sample p from U(0,1)\n",
    "        # compute s = self.gamma_icdf(p)\n",
    "        # sample u from U(-1,1)\n",
    "        # return u*s + self.loc\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        finfo = torch.finfo(self.loc.dtype)\n",
    "        if torch._C._get_tracing_state():\n",
    "            # [JIT WORKAROUND] lack of support for .uniform_()\n",
    "            u = torch.rand(shape, dtype=self.loc.dtype, device=self.loc.device) * 2 - 1\n",
    "            return self.loc - self.scale * u.sign() * torch.log1p(-u.abs().clamp(min=finfo.tiny))\n",
    "        u = self.loc.new(shape).uniform_(finfo.eps - 1, 1)\n",
    "        # TODO: If we ever implement tensor.nextafter, below is what we want ideally.\n",
    "        # u = self.loc.new(shape).uniform_(self.loc.nextafter(-.5, 0), .5)\n",
    "        return self.loc - self.scale * u.sign() * torch.log1p(-u.abs())\n",
    "\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        return (-torch.log(2 * self.scale) - torch.lgamma(1/self.beta) + torch.log(self.beta)\n",
    "                - torch.pow((torch.abs(value - self.loc) / self.scale), self.beta))\n",
    "\n",
    "\n",
    "    def cdf(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        c = (value - self.loc).sign() / 2\n",
    "        return 0.5 + c * self.gamma_cdf(value)\n",
    "\n",
    "\n",
    "    def icdf(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        term = value - 0.5\n",
    "        return term.sign() * self.gamma_icdf(2*term.abs()).pow(1/self.beta) + self.loc\n",
    "\n",
    "\n",
    "    def entropy(self):\n",
    "        return (1/self.beta) - torch.log(self.beta) + torch.log(2*self.scale) + torch.lgamma(1/self.beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
