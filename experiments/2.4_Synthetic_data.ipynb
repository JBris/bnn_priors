{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from bnn_priors.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomData:\n",
    "    \"\"\"\n",
    "    The usage is:\n",
    "    ```\n",
    "    data = RandomData(dim=64, n_points=2000)\n",
    "    ```\n",
    "    e.g. normalized training dataset:\n",
    "    ```\n",
    "    data.norm.train\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=100, n_points=2000, dtype='float32', device=\"cpu\"):\n",
    "        X_unnorm = t.from_numpy(np.random.uniform(low=-1., high=1., size=[n_points, dim]).astype(dtype))\n",
    "        y_unnorm = t.from_numpy(np.random.uniform(low=-1., high=1., size=[n_points, 1]).astype(dtype))\n",
    "\n",
    "        # split into train and test\n",
    "        index_train = np.arange(n_points//2)\n",
    "        index_test  = np.arange(n_points//2, n_points)\n",
    "\n",
    "        # record unnormalized dataset\n",
    "        self.unnorm = Dataset(X_unnorm, y_unnorm, index_train, index_test, device)\n",
    "\n",
    "        # compute normalization constants based on training set\n",
    "        self.X_std = t.std(self.unnorm.train_X, 0)\n",
    "        self.X_std[self.X_std == 0] = 1. # ensure we don't divide by zero\n",
    "        self.X_mean = t.mean(self.unnorm.train_X, 0)\n",
    "\n",
    "        self.y_mean = t.mean(self.unnorm.train_y)\n",
    "        self.y_std  = t.std(self.unnorm.train_y)\n",
    "\n",
    "        X_norm = (self.unnorm.X - self.X_mean)/self.X_std\n",
    "        y_norm = (self.unnorm.y - self.y_mean)/self.y_std\n",
    "\n",
    "        self.norm = Dataset(X_norm, y_norm, index_train, index_test, device)\n",
    "\n",
    "        self.num_train_set = self.unnorm.X.shape[0]\n",
    "        self.in_shape   = self.unnorm.X.shape[1:]\n",
    "        self.out_shape  = self.unnorm.y.shape[1:]\n",
    "\n",
    "    def denormalize_y(self, y):\n",
    "        return self.y_std * y + self.y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = RandomData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.norm.train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.norm.test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data based on BNN prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnn_priors import exp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthetic:\n",
    "    \"\"\"\n",
    "    The usage is:\n",
    "    ```\n",
    "    synth_data = Synthetic(dataset=data, model=net)\n",
    "    ```\n",
    "    e.g. normalized training dataset:\n",
    "    ```\n",
    "    data.norm.train\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, model, dtype='float32', device=\"cpu\"):\n",
    "        if batch_size is None:\n",
    "            new_y = model(dataset.norm.X).sample()\n",
    "        else:\n",
    "            dataloader_train = t.utils.data.DataLoader(dataset.norm.train, batch_size=batch_size)\n",
    "            dataloader_test = t.utils.data.DataLoader(dataset.norm.test, batch_size=batch_size)\n",
    "            batch_preds = []\n",
    "            for dataloader in [dataloader_train, dataloader_test]:\n",
    "                for batch_x, _ in dataloader:\n",
    "                    batch_preds.append(model(batch_x).sample())\n",
    "            new_y = t.cat(batch_preds)\n",
    "\n",
    "        # split into train and test\n",
    "        index_train = np.arange(len(dataset.norm.train_X))\n",
    "        index_test  = np.arange(len(dataset.norm.train_X), len(dataset.norm.X))\n",
    "\n",
    "        # record unnormalized dataset\n",
    "        self.unnorm = Dataset(dataset.unnorm.X, new_y, index_train, index_test, device)\n",
    "        self.norm = Dataset(dataset.norm.X, new_y, index_train, index_test, device)\n",
    "\n",
    "        self.num_train_set = self.unnorm.X.shape[0]\n",
    "        self.in_shape   = self.unnorm.X.shape[1:]\n",
    "        self.out_shape  = self.unnorm.y.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"densenet\"\n",
    "width = 50\n",
    "weight_prior = \"gaussian\"\n",
    "bias_prior = \"gaussian\"\n",
    "weight_loc = 0.\n",
    "weight_scale = 2.**0.5\n",
    "bias_loc = 0.\n",
    "bias_scale = 1.\n",
    "weight_prior_params = {}\n",
    "bias_prior_params = {}\n",
    "batchnorm = True\n",
    "device = \"cpu\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"random\"\n",
    "prior = \"laplace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = exp_utils.get_data(data, device)\n",
    "\n",
    "x_train = dataset.norm.train_X\n",
    "y_train = dataset.norm.train_y\n",
    "\n",
    "net = exp_utils.get_model(x_train=x_train, y_train=y_train, model=model, width=width, weight_prior=prior, weight_loc=weight_loc,\n",
    "                          weight_scale=weight_scale, bias_prior=bias_prior, bias_loc=bias_loc, bias_scale=bias_scale, batchnorm=batchnorm,\n",
    "                         weight_prior_params={}, bias_prior_params=bias_prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = Synthetic(dataset, net, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 100])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.norm.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.norm.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
