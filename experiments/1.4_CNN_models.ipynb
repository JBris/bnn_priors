{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bnn_priors.models import DenseNet\n",
    "from bnn_priors.data import CIFAR10\n",
    "from bnn_priors import prior\n",
    "from bnn_priors.models import RegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ResNet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, bn=True):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        if bn:\n",
    "            batchnorm = nn.BatchNorm2d\n",
    "        else:\n",
    "            batchnorm = nn.Identity\n",
    "        self.bn1 = batchnorm(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = batchnorm(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv2(F.relu(out))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, bn=True):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        if bn:\n",
    "            batchnorm = nn.BatchNorm2d\n",
    "        else:\n",
    "            batchnorm = nn.Identity\n",
    "        self.bn1 = batchnorm(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = batchnorm(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = batchnorm(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv2(F.relu(out))\n",
    "        out = self.bn3(out)\n",
    "        out = self.conv3(F.relu(out))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, bn=True):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.bn = bn\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, bn=self.bn))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(bn=True):\n",
    "    return PreActResNet(PreActBlock, [2,2,2,2], bn=bn)\n",
    "\n",
    "def PreActResNet34(bn=True):\n",
    "    return PreActResNet(PreActBlock, [3,4,6,3], bn=bn)\n",
    "\n",
    "def PreActResNet50(bn=True):\n",
    "    return PreActResNet(PreActBottleneck, [3,4,6,3], bn=bn)\n",
    "\n",
    "def PreActResNet101(bn=True):\n",
    "    return PreActResNet(PreActBottleneck, [3,4,23,3], bn=bn)\n",
    "\n",
    "def PreActResNet152(bn=True):\n",
    "    return PreActResNet(PreActBottleneck, [3,8,36,3], bn=bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR10()\n",
    "\n",
    "dataloader_train = DataLoader(data.norm.train, batch_size=32, shuffle=True, drop_last=True)\n",
    "dataloader_test = DataLoader(data.norm.test, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 7/1562 [00:00<00:38, 40.04it/s, loss=1.40]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test accuracy = 56.4 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 7/1562 [00:00<00:40, 38.02it/s, loss=1.18]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test accuracy = 69.6 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 7/1562 [00:00<00:39, 39.38it/s, loss=0.63]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Test accuracy = 77.6 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 7/1562 [00:00<00:39, 39.86it/s, loss=0.42]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Test accuracy = 77.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Test accuracy = 79.4 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "lr = 5e-4\n",
    "\n",
    "net = PreActResNet18(bn=True).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    with tqdm(desc=f\"Epoch {epoch}\", total=len(dataloader_train), leave=False) as pbar:\n",
    "        for batch_x, batch_y in dataloader_train:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = net(batch_x)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.update()\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.2f}\"})\n",
    "            \n",
    "    total_acc = 0.\n",
    "    num_batches = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader_test:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            y_pred = net(batch_x)\n",
    "            total_acc += y_pred.argmax(axis=1).eq(batch_y).float().mean().item()\n",
    "            num_batches += 1\n",
    "    acc = total_acc/num_batches\n",
    "    print(f\"Epoch {epoch}: Test accuracy = {acc*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bayesian CNNs analogous to the DenseNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, weight_prior, bias_prior=None):\n",
    "        nn.Module.__init__(self)\n",
    "        (self.out_features, self.in_features) = weight_prior.p.shape\n",
    "        self.weight_prior = weight_prior\n",
    "        self.bias_prior = bias_prior\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.weight_prior()\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return (None if self.bias_prior is None else self.bias_prior())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearPrior(in_dim, out_dim, prior_w=prior.Normal, loc_w=0., std_w=1.,\n",
    "                     prior_b=prior.Normal, loc_b=0., std_b=1., scaling_fn=None):\n",
    "    if scaling_fn is None:\n",
    "        def scaling_fn(std, dim):\n",
    "            return std/dim**0.5\n",
    "    return Linear(prior_w((out_dim, in_dim), loc_w, scaling_fn(std_w, in_dim)),\n",
    "                  prior_b((out_dim,), 0., std_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Conv2d):\n",
    "    def __init__(self, weight_prior, bias_prior=None, stride=1,\n",
    "            padding=0, dilation=1, groups=1, padding_mode='zeros'):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.stride = nn.modules.utils._pair(stride)\n",
    "        self.padding = nn.modules.utils._pair(padding)\n",
    "        self.dilation = nn.modules.utils._pair(dilation)\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        self.transposed = False\n",
    "        self.output_padding = nn.modules.utils._pair(0)\n",
    "        \n",
    "        (self.out_channels, in_channels, ksize_0, ksize_1) = weight_prior.p.shape\n",
    "        self.in_channels = in_channels * self.groups\n",
    "        self.kernel_size = (ksize_0, ksize_1)\n",
    "        self.weight_prior = weight_prior\n",
    "        self.bias_prior = bias_prior\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.weight_prior()\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return (None if self.bias_prior is None else self.bias_prior())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2dPrior(in_channels, out_channels, kernel_size, stride=1,\n",
    "            padding=0, dilation=1, groups=1, padding_mode='zeros',\n",
    "            prior_w=prior.Normal, loc_w=0., std_w=1., prior_b=prior.Normal,\n",
    "            loc_b=0., std_b=1., scaling_fn=None):\n",
    "    if scaling_fn is None:\n",
    "        def scaling_fn(std, dim):\n",
    "            return std/dim**0.5\n",
    "    kernel_size = nn.modules.utils._pair(kernel_size)\n",
    "    bias_prior = prior_b((out_channels,), 0., std_b) if prior_b is not None else None\n",
    "    return Conv2d(weight_prior=prior_w((out_channels, in_channels//groups, kernel_size[0], kernel_size[1]),\n",
    "                                       loc_w, scaling_fn(std_w, in_channels)),\n",
    "                  bias_prior=bias_prior,\n",
    "                 stride=stride, padding=padding, dilation=dilation,\n",
    "                  groups=groups, padding_mode=padding_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test = Conv2dPrior(3, 16, 3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(\n",
       "  3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "  (weight_prior): Normal()\n",
       "  (bias_prior): Normal()\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 32, 32])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_test(batch_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, bn=True,\n",
    "                 prior_w=prior.Normal, loc_w=0., std_w=2**.5,\n",
    "                 prior_b=prior.Normal, loc_b=0., std_b=1.,\n",
    "                scaling_fn=None):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        if bn:\n",
    "            batchnorm = nn.BatchNorm2d\n",
    "        else:\n",
    "            batchnorm = nn.Identity\n",
    "        self.bn1 = batchnorm(in_planes)\n",
    "        self.conv1 = Conv2dPrior(in_planes, planes, kernel_size=3, stride=stride, padding=1,\n",
    "                                 prior_w=prior_w, loc_w=loc_w, std_w=std_w,\n",
    "                                 prior_b=None, scaling_fn=scaling_fn)\n",
    "        self.bn2 = batchnorm(planes)\n",
    "        self.conv2 = Conv2dPrior(planes, planes, kernel_size=3, stride=1, padding=1,\n",
    "                                 prior_w=prior_w, loc_w=loc_w, std_w=std_w,\n",
    "                                 prior_b=None, scaling_fn=scaling_fn)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv2(F.relu(out))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, bn=True,\n",
    "                 prior_w=prior.Normal, loc_w=0., std_w=2**.5,\n",
    "                 prior_b=prior.Normal, loc_b=0., std_b=1.,\n",
    "                scaling_fn=None):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.bn = bn\n",
    "        self.prior_w = prior_w\n",
    "        self.loc_w = loc_w\n",
    "        self.std_w = std_w\n",
    "        self.prior_b = prior_b\n",
    "        self.loc_b = loc_b\n",
    "        self.std_b = std_b\n",
    "        self.scaling_fn = scaling_fn\n",
    "\n",
    "        self.conv1 = Conv2dPrior(3, 64, kernel_size=3, stride=1, padding=1, prior_b=None,\n",
    "                           prior_w=self.prior_w, loc_w=self.loc_w, std_w=self.std_w,\n",
    "                           scaling_fn=self.scaling_fn)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = LinearPrior(512*block.expansion, num_classes,\n",
    "                            prior_w=self.prior_w, loc_w=self.loc_w, std_w=self.std_w,\n",
    "                            prior_b=self.prior_b, loc_b=self.loc_b, std_b=self.std_b,\n",
    "                            scaling_fn=self.scaling_fn)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, bn=self.bn,\n",
    "                                prior_w=self.prior_w, loc_w=self.loc_w, std_w=self.std_w,\n",
    "                                prior_b=self.prior_b, loc_b=self.loc_b, std_b=self.std_b,\n",
    "                                scaling_fn=self.scaling_fn))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(noise_std=1.,\n",
    "             prior_w=prior.Normal, loc_w=0., std_w=2**.5,\n",
    "             prior_b=prior.Normal, loc_b=0., std_b=1.,\n",
    "            scaling_fn=None, bn=True):\n",
    "    return RegressionModel(PreActResNet(PreActBlock,\n",
    "                                        [2,2,2,2], bn=bn,\n",
    "                                        prior_w=prior_w,\n",
    "                                       loc_w=loc_w,\n",
    "                                       std_w=std_w,\n",
    "                                       prior_b=prior_b,\n",
    "                                       loc_b=loc_b,\n",
    "                                       std_b=std_b,\n",
    "                                       scaling_fn=scaling_fn,), noise_std)\n",
    "\n",
    "def PreActResNet34(noise_std=1.,\n",
    "             prior_w=prior.Normal, loc_w=0., std_w=2**.5,\n",
    "             prior_b=prior.Normal, loc_b=0., std_b=1.,\n",
    "            scaling_fn=None, bn=True):\n",
    "    return RegressionModel(PreActResNet(PreActBlock,\n",
    "                                        [3,4,6,3], bn=bn,\n",
    "                                        prior_w=prior_w,\n",
    "                                       loc_w=loc_w,\n",
    "                                       std_w=std_w,\n",
    "                                       prior_b=prior_b,\n",
    "                                       loc_b=loc_b,\n",
    "                                       std_b=std_b,\n",
    "                                       scaling_fn=scaling_fn,), noise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_test = PreActResNet18(bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionModel(\n",
       "  (net): PreActResNet(\n",
       "    (conv1): Conv2d(\n",
       "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (weight_prior): Normal()\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): Identity()\n",
       "        (conv1): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (weight_prior): Normal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(\n",
       "      in_features=512, out_features=10, bias=True\n",
       "      (weight_prior): Normal()\n",
       "      (bias_prior): Normal()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2338e+08,  2.4759e+07,  5.2194e+07, -2.1589e+07, -9.9581e+07,\n",
       "          5.3191e+07,  3.0906e+07, -4.0408e+07, -1.6005e+07,  7.4893e+07],\n",
       "        [-1.1931e+08,  1.0338e+07,  3.8562e+07, -4.2010e+07, -8.4916e+07,\n",
       "          5.5311e+07,  8.9196e+06, -3.5700e+06, -4.6934e+06,  6.2197e+07],\n",
       "        [-2.0265e+08,  2.5444e+07,  5.9272e+07, -6.0113e+07, -1.4750e+08,\n",
       "          1.0304e+08,  6.5790e+07, -1.4920e+07, -8.4617e+05,  9.4435e+07],\n",
       "        [-1.9261e+08,  3.0068e+07,  6.2641e+07, -6.2738e+07, -1.3065e+08,\n",
       "          1.3270e+08,  4.1909e+07, -4.4233e+07, -2.7670e+07,  1.1862e+08],\n",
       "        [-1.3359e+08,  1.0180e+07,  3.4459e+07, -3.2965e+07, -8.7067e+07,\n",
       "          7.0907e+07,  2.6929e+07, -1.4885e+07, -1.8933e+07,  7.6678e+07],\n",
       "        [-1.9609e+08,  3.5271e+07,  6.6672e+07, -4.2920e+07, -1.6181e+08,\n",
       "          9.5549e+07,  4.1004e+07, -3.4805e+07, -1.2914e+07,  8.2691e+07],\n",
       "        [-1.3120e+08,  2.5098e+07,  4.3022e+07, -3.4870e+07, -1.1319e+08,\n",
       "          5.9634e+07,  3.6942e+07, -1.9157e+06, -4.0601e+06,  6.7957e+07],\n",
       "        [-1.5669e+08,  2.4577e+07,  4.2984e+07, -4.5958e+07, -1.3031e+08,\n",
       "          8.8720e+07,  1.1979e+07, -2.5697e+07, -1.7816e+07,  8.5966e+07],\n",
       "        [-1.5072e+08,  4.2084e+07,  2.6235e+07, -3.3456e+07, -1.0842e+08,\n",
       "          6.2745e+07,  3.2844e+07, -1.6710e+06, -2.1860e+07,  6.6684e+07],\n",
       "        [-1.2303e+08,  2.7146e+07,  3.5470e+07, -2.1826e+07, -1.0579e+08,\n",
       "          4.7976e+07,  2.0515e+07,  3.6291e+06, -2.1264e+07,  7.7976e+07],\n",
       "        [-9.8766e+07,  4.0400e+06,  2.4252e+07, -3.2447e+07, -8.9627e+07,\n",
       "          5.9271e+07,  2.4053e+07, -1.8796e+07, -6.1446e+06,  6.5526e+07],\n",
       "        [-1.9782e+08,  1.6320e+07,  5.0828e+07, -6.2103e+07, -1.4189e+08,\n",
       "          1.2576e+08,  4.0360e+07, -2.7830e+07, -1.4901e+07,  9.6854e+07],\n",
       "        [-1.7475e+08,  1.4708e+07,  6.4151e+07, -5.3477e+07, -1.3290e+08,\n",
       "          9.5407e+07,  3.1800e+07, -2.4951e+07,  3.9197e+06,  9.7761e+07],\n",
       "        [-1.2317e+08,  2.8579e+07,  3.8991e+07, -5.3671e+07, -7.9799e+07,\n",
       "          7.6458e+07,  2.7794e+07,  1.3864e+07,  8.2398e+06,  6.0434e+07],\n",
       "        [-1.5044e+08,  2.9208e+07,  3.2537e+07, -3.4690e+07, -1.0773e+08,\n",
       "          6.9047e+07,  1.4446e+07,  1.1488e+06, -1.0760e+07,  6.6790e+07],\n",
       "        [-1.4249e+08,  3.0457e+07,  2.6947e+07, -3.7104e+07, -1.1681e+08,\n",
       "          4.7508e+07,  1.7671e+07,  4.2812e+06, -2.7287e+06,  6.2900e+07],\n",
       "        [-1.1533e+08,  2.4875e+07,  1.2736e+07, -3.4535e+07, -8.2345e+07,\n",
       "          4.6177e+07,  1.8465e+07,  7.6802e+06, -3.4279e+06,  4.2744e+07],\n",
       "        [-1.0886e+08,  1.4241e+07,  3.1582e+07, -2.0364e+07, -8.6804e+07,\n",
       "          5.5177e+07,  1.5983e+07, -5.7894e+06, -1.8917e+07,  6.2806e+07],\n",
       "        [-1.7992e+08,  3.3655e+07,  5.6780e+07, -5.7033e+07, -1.3204e+08,\n",
       "          8.9519e+07,  3.5095e+07, -1.7560e+06, -1.5913e+07,  9.9673e+07],\n",
       "        [-1.2825e+08,  2.0555e+07,  4.2160e+07, -3.2659e+07, -1.1861e+08,\n",
       "          6.3393e+07,  3.6369e+07,  4.0898e+06, -6.4946e+05,  7.0689e+07],\n",
       "        [-1.3081e+08,  1.7482e+07,  4.4832e+07, -4.9966e+07, -1.0545e+08,\n",
       "          7.0410e+07,  2.6658e+07, -8.6924e+05, -2.6410e+07,  6.2150e+07],\n",
       "        [-1.3480e+08, -2.2511e+06,  4.2089e+07, -3.9237e+07, -9.6643e+07,\n",
       "          8.9363e+07,  3.6082e+07, -2.1899e+07, -2.3467e+07,  6.2160e+07],\n",
       "        [-1.1684e+08,  1.5750e+07,  4.1232e+07, -3.8348e+07, -8.4386e+07,\n",
       "          8.1769e+07,  2.6014e+07, -3.1346e+07, -1.8540e+07,  6.4561e+07],\n",
       "        [-1.5031e+08,  3.5398e+07,  3.0205e+07, -5.5098e+07, -1.0539e+08,\n",
       "          8.0586e+07,  2.7780e+07,  5.8382e+06, -1.3783e+07,  7.9253e+07],\n",
       "        [-1.6563e+08,  1.6790e+07,  5.1735e+07, -7.5056e+07, -1.1903e+08,\n",
       "          8.8159e+07,  3.9918e+07, -1.2777e+07, -2.8343e+07,  7.5802e+07],\n",
       "        [-2.3371e+08,  7.9557e+06,  6.2080e+07, -6.6125e+07, -2.0510e+08,\n",
       "          1.2676e+08,  6.5196e+07, -6.0899e+07, -3.0720e+07,  1.3498e+08],\n",
       "        [-1.1051e+08,  6.7200e+06,  4.2484e+07, -2.5367e+07, -8.1516e+07,\n",
       "          5.8134e+07,  2.4678e+07, -4.3617e+06, -8.9842e+06,  5.9494e+07],\n",
       "        [-6.5501e+07,  7.8809e+06,  2.9807e+07, -1.8078e+07, -4.5927e+07,\n",
       "          3.0237e+07,  9.1553e+06, -1.0783e+07,  2.2488e+06,  2.2815e+07],\n",
       "        [-1.8022e+08,  5.1331e+07,  5.7089e+07, -2.7017e+07, -1.3310e+08,\n",
       "          1.0213e+08,  4.1781e+07,  6.9681e+06, -8.2262e+06,  1.0609e+08],\n",
       "        [-9.4992e+07,  2.7470e+07,  1.0239e+07, -2.1351e+06, -7.0001e+07,\n",
       "          3.2425e+07,  1.6430e+07, -6.6277e+06, -1.1805e+07,  5.0634e+07],\n",
       "        [-2.0035e+08,  5.0617e+07,  5.9740e+07, -5.3123e+07, -1.5393e+08,\n",
       "          9.6328e+07,  6.3555e+07, -2.0543e+07, -2.8453e+07,  1.1131e+08],\n",
       "        [-2.1404e+08,  1.5244e+07,  5.4714e+07, -6.2500e+07, -1.7388e+08,\n",
       "          1.1510e+08,  6.2582e+07, -2.6020e+07, -1.0080e+07,  1.1102e+08]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_test(batch_x).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
